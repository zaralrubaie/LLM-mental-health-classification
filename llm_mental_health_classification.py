# -*- coding: utf-8 -*-
"""
LLM mental health classification

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/16Ryhn8WmFloFnQJLlF4Gv1wPdBB5dryY

This script fine-tunes a DistilBERT model to classify mental health text into
three categories: normal, depression, and anxiety.
It also provides a Gradio interface for interactive predictions.

Steps:
1. Load and preprocess data
2. Assign labels
3. Tokenize text
4. Create PyTorch dataset
5. Train DistilBERT with class weights
6. Evaluate model
7. Deploy with Gradio app
"""
import torch
import pandas as pd
from transformers import Trainer
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score
import numpy as np
from transformers import TrainingArguments
import gradio as gr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
# 1. Load and preprocess dataset
df = pd.read_csv('/content/train.csv')  # Load dataset
df = df.dropna()  # Drop missing values

# Combine "Context" and "Response" into a single text colum
df["text"] = df["Context"] + " " + df["Response"]

# 2. Assign labels based on keywords
def assign_label(text):
    text = text.lower()
    if "depression" in text or "worthless" in text or "sad" in text:
        return "depression"
    elif "anxiety" in text or "stress" in text:
        return "anxiety"
    else:
        return "normal"

df["label"] = df["text"].apply(assign_label)
df['label'].value_counts()

# Drop unused columns
df=df.drop(['Context','Response'],axis=1)
# Encode labels numerically
le=LabelEncoder()
df['label']=le.fit_transform(df['label'])

# split data into train and test and val sets
X = df["text"]
y = df["label"]

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)
num_labels=df['label'].nunique()

# 3. Tokenizer & Model
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

train_texts = X_train.tolist()
val_texts   = X_val.tolist()
test_texts  = X_test.tolist()

# Tokenize text datasets
train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=64
)
val_encodings = tokenizer(
    val_texts,
    truncation=True,
    padding=True,
    max_length=64
)
test_encodings = tokenizer(
    test_texts,
    truncation=True,
    padding=True,
    max_length=64
)
# 4. Custom PyTorch Dataset
class MentalHealthDataset(torch.utils.data.Dataset):
    """Custom Dataset class for tokenized text and labels."""
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item
        
# Create datasets
train_dataset = MentalHealthDataset(train_encodings, y_train.tolist())
val_dataset   = MentalHealthDataset(val_encodings, y_val.tolist())
test_dataset  = MentalHealthDataset(test_encodings, y_test.tolist())

# 5. Class Weights for Imbalanced Data
y_train_labels = y_train.to_numpy()
classes = np.array([0, 1, 2])
weights = compute_class_weight(class_weight="balanced", classes=classes, y=y_train_labels)
class_weights = torch.tensor(weights, dtype=torch.float)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
class_weights = class_weights.to(device)

print("Class weights:", class_weights)

# 6. Custom Trainer with Weighted Loss
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels").to(device)
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}
    
# Training hyperparameters
training_args = TrainingArguments(
    output_dir="./results",              # directory to save model
    num_train_epochs=3,                  # number of epochs
    per_device_train_batch_size=16,      # batch size per device
    per_device_eval_batch_size=16,       # batch size for evaluation
    learning_rate=2e-5,                  # learning rate
    weight_decay=0.01,                   # optional weight decay
    logging_dir="./logs",                # directory for logs
    logging_steps=10,
    save_total_limit=1,
    report_to=[]                         
)
# Initialize trainer
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
# Train and evaluate
trainer.train()
val_results = trainer.evaluate(val_dataset)
print("Validation Results:", val_results)

test_results = trainer.evaluate(test_dataset)
print("Test Results:", test_results)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

# 7. Gradio App for Predictions
id2label = {0: "normal", 1: "depression", 2: "anxiety"}

def predict_mental_health(text):
 """
    Predict the mental health category for input text.
    Returns both label and class probabilities.
    """
    if isinstance(text, str):
        texts = [t.strip() for t in text.split("\n") if t.strip()]
    else:
        texts = list(text)

    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )
    enc = {k: v.to(device) for k, v in enc.items()}

# Run inference
 with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()
# Format results
 results = []
    for prob in probs:
        pred_idx = np.argmax(prob)
        label = id2label.get(pred_idx, f"unknown({pred_idx})")
        prob_dict = {id2label[i]: float(prob[i]) for i in range(len(prob))}
        results.append((label, prob_dict))

    return results if len(results) > 1 else results[0]
# Example inputs
examples = [
    "I'm feeling really down today, nothing seems to help.",
    "I feel happy and relaxed after my vacation!",
    "I can't sleep and keep worrying about work.",
]
# gradio interface 
iface = gr.Interface(
    fn=predict_mental_health,
    inputs=gr.Textbox(
        lines=5,
        placeholder="Enter one or more messages, one per line..."
    ),
    outputs=[gr.Textbox(label="Predicted Label"), gr.Label(num_top_classes=3, label="Class Probabilities")],
    examples=examples,
    title="ðŸ§  Mental Health Prediction",
    description=
"""
Enter one or more messages (one per line) and see the predicted mental health category along with class probabilities.
- 3 classes: normal, depression, anxiety
"""
)

iface.launch()
